#title Pig 笔记

* 安装

svn 代码：
<example>
$ svn co http://svn.apache.org/repos/asf/hadoop/pig/trunk
</example>

发布版本下载 http://www.apache.org/dyn/closer.cgi/hadoop/pig

* 运行

需要设置环境变量：
<example>
export HADOOP_HOME=/path/to/hadoop-version
export PIGDIR=/path/to/pig
export PIG_CLASSPATH=$PIGDIR/pig.jar:$HADOOP_HOME/conf
</example>

运行 grunt shell：
<example>
$PIGDIR/bin/pig -x local
</example>

运行本地脚本：
<example>
$PIGDIR/bin/pig -x local local.pig
</example>

运行 hadoop 脚本：
<example>
java -cp $PIG_CLASSPATH org.apache.pig.Main hadoop.pig
</example>

* Pig Cookbook
** 使用最新的代码
** 使用类型
默认数字类型是double。如果不需要高精度，可以使用 integer 或 long 类型。
这通加快算术运算，也能在早期发现错误。

** 使用需要的字段
如果一些字段没有用，可以在早期去除这些字段。
<example>
A = load 'myfile' as (t, u, v);
A1 = foreach A generate t, u;
</example>
** 尽早过滤
** 减少不必要的管道操作
** 使用算术类型 UDF
** Join 前去除 NULL
** 使用 Join 优化
join 表格的最后一个是从 stream 导入而不是加载到内存，所以把大表格放到
最后一个导入。
<example>
small = load 'small_file' as (t, u, v);
large = load 'large_file' as (x, y, z);
C = join small by t, large by x;
</example>
** 使用 replicate join
** 使用 PARALLEL
** 使用 LIMIT
** 使用 DISTINCT 而不是 GROUP BY - GENERATE

* Pig UDF (用户自定义函数)
** 简单的 Eval 函数

<example>
-- myscript.pig
REGISTER myudfs.jar;
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = FOREACH A GENERATE myudfs.UPPER(name);
DUMP B;
</example>

第一行 REGISTER 注册 UDF 定义的 jar 文件。这个文件需要能在 classpath
中找到，或者是绝对路径的文件，或者是相对 pig 启动时的相对路径。如果未
找到 jar 文件将报错 =java.io.IOException: Can't read jar file:
myudfs.jar= 。

可以注册多个文件，如果有相同的函数，根据 java 规则是使用第一次出现的函
数。

UDF 必须使用带 package 的全称，否则会报函数找不到的错误
=java.io.IOException: Cannot instantiate:UPPER= 。

<src lang="java">
package myudfs;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.util.WrappedIOException;

public class UPPER extends EvalFunc (String)
{
    public String exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try{
            String str = (String)input.get(0);
            return str.toUpperCase();
        }catch(Exception e){
            throw WrappedIOException.wrap("Caught exception processing input row ", e);
        }
    }
}
</src>

* Pig Latin

** 语句 (Statement)
Pig Latin 语句是以一个 relation 作输入产生另一个 relation (LOAD 和
STORE 语句除外)。

pig 执行过程：
 1. 检查语法和语义
 2. 当遇到 DUMP 或 STORE 时，PIG 执行和这个 DUMP 或 STORE 相关的所有语
句。

下面这个例子，PIG 会检查语句，但是不会执行：
<example>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
</example>

** 注释
多行注释使用 <literal>/* */</literal> 。
单行注释使用 <literal>--</literal> 。

** Relations, Bags, Tuples, Fields

 1. relation 是 bag, 更精确的说是一个 outer bag
 2. bag 是 tuple 集合
 3. tuple 是一个有序的 fields
 4. field 是一块数据

pig 的 relation 与关系数据库中的关系表很相似，但是不需要每个 tuple 的
列数相同，也不需要 tuple 内的数据类型一致。

relation 是无序的，所以不能保证 tuple 是以特定顺序处理（而且很可能是并行处
理的）。

relation 是使用名字（或别名）引用。名字是在 Pig Latin 语句中进行指定。比如
下面这个例子中定义的关系 A：
<example>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
DUMP A;
(John,18,4.0F)
(Mary,19,3.8F)
(Bill,20,3.9F)
(Joe,18,3.8F)
</example>

field 使用位置或者名字来引用：
 1. 位置由系统产生，使用 $ 加数字的形式，从0开始，比如：$0, $1, $2
 2. 名字是使用 schema 时指定。可以使用任何不是 pig 关键字的名字

比如下面这个关系：
|                                                | First Field | Second Field | Third Field |
| Data type                                      | chararray   | int          | float       |
| Positional notation (generated by system)      | $0          | $1           | $2          |
| Possible name (assigned by you using a schema) | name        | age          | gpa         |
| Field value (for the first tuple)              | John        | 18           | 4.0         |

如果使用位置形式取 field，数组越界时将在执行前报错
=java.io.IOException: Out of bound access. Trying to access
non-existent  : 3. Schema {f1: bytearray,f2: bytearray,f3: bytearray}
has 3 column(s). etc …= 。

tuple 的 field 可以是任意的数据类型，包括复杂类型： bag, tuple, map。
复杂的 fields 可以使用复杂数据类型的 schema 产生。比如：
<example>
cat data;
(3,8,9) (4,5,6)
(1,4,7) (3,7,5)
(2,5,8) (9,5,8)
A = LOAD 'data' AS (t1:tuple(t1a:int, t1b:int,t1c:int),t2:tuple(t2a:int,t2b:int,t2c:int));
DUMP A;
((3,8,9),(4,5,6))
((1,4,7),(3,7,5))
((2,5,8),(9,5,8))
X = FOREACH A GENERATE t1.t1a,t2.$0;
DUMP X;
(3,4)
(1,3)
(2,9)
</example>

relation 和 field 的别名都是大小写敏感的。Pig Latin 函数也是这样。但是
参数和 Pig Latin 关键字都是大小写不敏感的。例如：
 1. 关系名 A,B,C 都是大小写敏感
 2. 关系名字段 f1, f2, f3 都是大小写敏感
 3. 函数 PigStorage 和 COUNT是大小写敏感
 4. 关键字 LOAD, USING, AS, GROUP, BY, FOREACH, GENERATE, DUMP 都是大
小写不敏感。

** 处理数据
通常数据处理方式包括：
 1. 使用 FILTER 操作 tuple 或者称为 row。
 2. 使用 FOREACH 处理 field 或 column
 3. 使用 GROUP 将数据分组。
 4. 使用 COGROUP 和 JOIN 将两组或多组数据合并
 5. 使用 UNION 合并两个或多个关系。
 6. 使用 SPLIT 操作将一个关系分解成多个关系

在 COGROUP, CROSS, DISTINCT, GROUP, JOIN, ORDER 操作时使用 PARALLEL 关
键字使操作并行进行。 PARALLEL 控制并行的 reducer 数量。

使用 DUMP 将数据显示到屏幕，使用 STORE 将数据写入到文件系统。

使用 DESCRIBE 可以显示关系的 schema。
使用 EXPLAIN 操作可以查看 逻辑、物理、map reduce 执行计划。
使用 ILLUSTRATE 查看语句每一步的执行。

数据类型：
 1. 标量类型： int，long，float, double
 2. 数组类型： chararray, bytearray
 3. 复杂数据类型：tuple, bag, map

数据类型一般在 schema 中指定，如果没有指定默认是 bytearray 类型。在操
作过程中可能会根据上下文发现隐式的数据类型转换。比如：
<example>
A = LOAD 'data' AS (f1,f2,f3);
B = FOREACH A GENERATE f1 + 5;
C = FOREACH A generate f1 + f2;
</example>

在 B 中 f1 转换在整数，而在 C 中 f1, f2 转换成 double。

如果在 LOAD 过程中数据类型不一致，将产生 null 或者错误。

如果显示的类型转换失败将产生错误，比如：
<example>
A = LOAD 'data' AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE (int)name;
</example>

如果隐式转换时类型不兼容，也将产生错误，比如：
<example>
A = LOAD 'data' AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name + gpa;
</example>

** COGROUP
COGROUP 将两组或多组关系按相同的字段合并成这新的结构 (join_field, A, B)。语法如下：
<example>
alias = COGROUP alias BY field_alias [INNER|OUTER], alias BY field_alias[INNER|OUTER] [PARALLEL n]; 
</example>
 - alias 是关系名。
 - field_alias 是关系中的一个或多个字段。当使用多个字段时，两个关系中的字段数必须一致。

<example>
A = LOAD 'data1' AS (owner:chararray,pet:chararray);
DUMP A;
(Alice,turtle)
(Alice,goldfish)
(Alice,cat)
(Bob,dog)
(Bob,cat)
B = LOAD 'data2' AS (friend1:chararray,friend2:chararray);
DUMP B;
(Cindy,Alice)
(Mark,Alice)
(Paul,Bob)
(Paul,Jane)
X = COGROUP A BY owner, B BY friend2;
DESCRIBE X;
X: {group: chararray,A: {owner: chararray,pet: chararray},B: {firend1:chararray,friend2: chararray}}
(Alice,{(Alice,turtle),(Alice,goldfish),(Alice,cat)},{(Cindy,Alice),(Mark,Alice)})
(Bob,{(Bob,dog),(Bob,cat)},{(Paul,Bob)})
(Jane,{},{(Paul,Jane)})
</example>

** CROSS
计算关系的笛卡尔积。
<example>
alias = CROSS alias, alias[, alias ...] [PARALLEL n];
</example>

<example>
A = LOAD 'data1' AS (a1:int,a2:int,a3:int);
DUMP A;
(1,2,3)
(4,2,1)
B = LOAD 'data2' AS (b1:int,b2:int);
DUMP B;
(2,4)
(8,9)
(1,3)
X = CROSS A, B;
DUMP X;
(1,2,3,2,4)
(1,2,3,8,9)
(1,2,3,1,3)
(4,2,1,2,4)
(4,2,1,8,9)
(4,2,1,1,3)
</example>

** DISTINCT

语法：
<example>
alias = DISTINCT alias [PARALLEL n];
</example>

<example>
A = LOAD 'data' AS (a1:int,a2:int,a3:int);
DUMP A;
(8,3,4)
(1,2,3)        
(4,3,3)        
(4,3,3)        
(1,2,3)
X = DISTINCT A;
DUMP X;
(1,2,3)
(4,3,3)
(8,3,4)
</example>

** FILTER

<example>
alias = FILTER alias BY expression; 
</example>

<example>
A = LOAD 'data' AS (a1:int,a2:int,a3:int);
DUMP A;
(1,2,3)
(4,2,1)
(8,3,4)
(4,3,3)
(7,2,5)
(8,4,3)
X = FILTER A BY f3 == 3;
DUMP X;
(1,2,3)
(4,3,3)
(8,4,3)
</example>

** FOREACH ... GENERATE ..

<example>
alias = FOREACh { gen_blk | nested_gen_blk } [ AS schema ];
</example>
alias 关系名
gen_blk 语法结构为
<example>
FOREACH alias GENERATE expression[, expression ...]
</example>
nested_gen_blk 语法结构为：
<example>
FOREACH nested_alias {
   alias = nested_op; [ alias = nested_op; .. ]
   GENERATE expression [ expression ... ]
};
</example>
nested_op 是

** DUMP
<example>
DUMP alias;
</example>
